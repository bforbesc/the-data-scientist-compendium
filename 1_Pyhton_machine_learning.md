# [![My Skills](https://skills.thijs.gg/icons?i=py)](https://skills.thijs.gg) Python - Machine Learning

# üìñ books
- [Machine learning refined](https://jermwatt.github.io/machine_learning_refined/)

# üî® resources and tools
- [Scikit learn official guide](https://scikit-learn.org/stable/user_guide.html)
- [An introduction to machine learning with scikit-learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)
- [Guide to choosing the right algorithm/ estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/)
- Reinforcement learning:
	- [The essentials of Reinforcement Learning](https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292)
	- [A guide on Reinforcement Learning](https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/)
- [Harvard CS 181: Machine Learning](https://harvard-ml-courses.github.io/cs181-web/): lecture recaps and notes
- [Machine learning operations principles](https://ml-ops.org/content/mlops-principles)
- [Notes on data science & machine learning](https://chrisalbon.com/): ‚ÄºÔ∏èincredible resource‚ÄºÔ∏è for Python snippets for data science and machine learning in general
- [Understanding the bias-variance tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)
- Understanding concepts with examples:
	- [Underfitting vs. overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html)
	- [Understanding ROC curves](http://www.navan.name/roc/)
	- [Comparing different clustering algorithms](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)
	- [Understanding UMPA](https://pair-code.github.io/understanding-umap/)
- https://towardsdatascience.com/the-surprising-behaviour-of-distance-metrics-in-high-dimensions-c2cb72779ea6

- [The People + AI Guidebook](https://pair.withgoogle.com/guidebook/): methods, best practices and examples for designing better AI
- [Explained AI: gradient boosting](https://explained.ai/gradient-boosting/): great and practical explanation of gradient boosting
- [CatBoost vs. Light GBM vs. XGBoost](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)
- [Stacking vs. Bagging vs. Boosting](https://mksaad.wordpress.com/2019/12/21/stacking-vs-bagging-vs-boosting/)
- [Guide to hyperparameter tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
- [How to do cross-validation when upsampling data](https://kiwidamien.github.io/how-to-do-cross-validation-when-upsampling-data.html)
- [Which machine mearning algorithms require feature scaling (standardization and normalization)?](https://www.kaggle.com/getting-started/159643)
- [Which machine mearning algorithms require one-hot encoding?](https://stats.stackexchange.com/questions/288095/what-algorithms-require-one-hot-encoding)
- [Which machine mearning algorithms require scaling, besides SVM?](https://stats.stackexchange.com/questions/244507/what-algorithms-need-feature-scaling-beside-from-svm)
- [5 SMOTE techniques for oversampling your imbalanced data](https://towardsdatascience.com/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5)


- [Shap: Shapley additive explanations](https://towardsdatascience.com/a-novel-approach-to-feature-importance-shapley-additive-explanations-d18af30fc21b)
- [Dive into Machine Learning](https://github.com/metjush/dive-into-machine-learning): an (old but interesting) guide on machine learning and data science
- [Machine learning algorithms' categories guide](https://chart-studio.plotly.com/create/?fid=SolClover%3A40&utm_source=pocket_mylist#/)



## Formulas/ reminders

Classification metrics

$\text{True positive rate} = \text{Recall} = \text{Sensitivity}$

$\text{True negative rate} = \text{Specificity} = \dfrac{TN}{TN + FP}$

$\text{False positive rate} = 1 - \text{Specificity}$

<br>

Data processing
> General rule: balancing > scaling > selection

> All models have tendency to overfit to some extent: ‚ÄúIf you torture the data long enough, it will confess.‚Äù (Nobel Laureate Ronald Coase)

> Logistic regression: ‚ÄúAll else being equal, if your income had been $20,000 higher you would have been granted this particular mortgage.‚Äù

> KNN: ‚ÄúWe declined your mortgage application because you remind us of the Smiths and the Mitchells, who both defaulted.‚Äù



## Building machine learning web apps
- [How to Deploy your data science Projects as web apps easily with Python](https://towardsdatascience.com/how-to-deploy-your-data-science-as-web-apps-easily-with-python-955dd462a9b5)
- [How to Build a Data Science Web App in Python (Penguin Classifier)](https://towardsdatascience.com/how-to-build-a-data-science-web-app-in-python-penguin-classifier-2f101ac389f3)
- [How to write Web apps using simple Python for Data Scientists?](https://towardsdatascience.com/how-to-write-web-apps-using-simple-python-for-data-scientists-a227a1a01582)

